# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ZJ9aoN8Fmc1iCK2YJSn6kDtw0lrG0jq

#Item Demand Forecasting
Description

This model helps the delivery company predict the demand on each product so it provide sufficient trucks.

The dataset is 5 years of store-item sales data.

Evaluation

Submissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.

Variables:

date
store
item
sales
"""

# Import necessary libraries

# Base
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Model
import lightgbm as lgb
import shap
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error
from lightgbm import early_stopping
from tensorflow import keras

# Configuration
import warnings
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)

"""#Load the dataset"""

df = pd.read_csv('./train.csv', parse_dates=['date'])

print(df.shape, "\n")
df.head()

"""# Histogram: Sales distribution across stores

"""

fig, axes = plt.subplots(2, 5, figsize=(20, 10))
for i in range(1,11):
    if i < 6:
        df[df.store == i].sales.hist(ax=axes[0, i-1])
        axes[0,i-1].set_title("Store " + str(i), fontsize = 15)

    else:
        df[df.store == i].sales.hist(ax=axes[1, i - 6])
        axes[1,i-6].set_title("Store " + str(i), fontsize = 15)
plt.tight_layout(pad=4.5)
plt.suptitle("Histogram: Sales");

"""# Feature Engineering

## Overview  
I created several new features, mainly related to time-based attributes, lag features, rolling statistics, and clustering. These features help capture seasonality, trends, and relationships within the data to enhance predictive modeling.

## Time-Related Features  
- **`month`, `day_of_month`, `day_of_year`, `week_of_year`, `day_of_week`, `year`**: Extracts different components from the date to capture seasonal and time-based trends.  
- **`is_wknd`**: Identifies whether a date falls on a weekend (Saturday/Sunday).  
- **`quarter`**: Identifies the quarter of the year (1 to 4).  
- **`is_month_start`, `is_month_end`, `is_quarter_start`, `is_quarter_end`, `is_year_start`, `is_year_end`**: Binary indicators for whether a date is the start or end of a month, quarter, or year.  
- **`season`**: Categorizes months into four seasons (Winter, Spring, Summer, Fall) to capture seasonal effects.  

## Lagged Features  
- **`sales_lag_1`, `sales_lag_7`, `sales_lag_30`**: Sales from 1, 7, and 30 days ago to capture short-term and long-term trends.  

## Rolling Window Features (Smoothed Sales Trends)  
- **`sales_roll_mean_7`, `sales_roll_mean_14`, `sales_roll_mean_30`**: Rolling averages of sales over 7, 14, and 30 days to smooth out fluctuations and capture trends.  

## Exponentially Weighted Moving Average (EWMA) Features  
- **`sales_ewm_alpha_X_lag_Y`**: Exponentially weighted moving average of sales with different smoothing factors (`alphas`) and lags. Helps in trend detection with more weight given to recent observations.  

## Clustering-Based Features (Grouping Stores and Items by Sales)  
- **`store_cluster`**: Assigns stores into 4 clusters based on their average sales using binning.  
- **`item_cluster`**: Assigns items into 5 clusters based on their average sales using binning.  

"""

# Feature Engineering

df['month'] = df.date.dt.month
df['day_of_month'] = df.date.dt.day
df['day_of_year'] = df.date.dt.dayofyear
df['week_of_year'] = df.date.dt.isocalendar().week
df['day_of_week'] = df.date.dt.dayofweek + 1
df['year'] = df.date.dt.year
df["is_wknd"] = df.date.dt.weekday >= 5
df["quarter"] = df.date.dt.quarter
df['is_month_start'] = df.date.dt.is_month_start.astype(int)
df['is_month_end'] = df.date.dt.is_month_end.astype(int)
df['is_quarter_start'] = df.date.dt.is_quarter_start.astype(int)
df['is_quarter_end'] = df.date.dt.is_quarter_end.astype(int)
df['is_year_start'] = df.date.dt.is_year_start.astype(int)
df['is_year_end'] = df.date.dt.is_year_end.astype(int)

# 0: Winter, 1: Spring, 2: Summer, 3: Fall
df["season"] = np.where(df.month.isin([12,1,2]), 0, 1)
df["season"] = np.where(df.month.isin([6,7,8]), 2, df["season"])
df["season"] = np.where(df.month.isin([9, 10, 11]), 3, df["season"])

df.sort_values(["date"], inplace=True)


# Lag Features (Sales from previous days)
for lag in [1, 7, 30]:
    df[f'sales_lag_{lag}'] = df['sales'].shift(lag)

# Rolling Features (Mean, Std, Min, Max, Sum)
for window in [7, 14, 30]:
    df[f'sales_roll_mean_{window}'] = df['sales'].rolling(window).mean().shift(1)
    # df[f'sales_roll_std_{window}'] = df['sales'].rolling(window).std().shift(1)
    # df[f'sales_roll_min_{window}'] = df['sales'].rolling(window).min().shift(1)
    # df[f'sales_roll_max_{window}'] = df['sales'].rolling(window).max().shift(1)
    # df[f'sales_roll_sum_{window}'] = df['sales'].rolling(window).sum().shift(1)

def ewm_features(dataframe, alphas, lags):
    dataframe = dataframe.copy()
    for alpha in alphas:
        for lag in lags:
            dataframe[f'sales_ewm_alpha_{str(alpha).replace(".", "")}_lag_{lag}'] = \
                dataframe['sales'].shift(lag).ewm(alpha=alpha).mean()
    return dataframe


alphas = [0.95, 0.9, 0.8, 0.7, 0.5]
lags = [91, 98, 105, 112, 180, 270, 365, 546, 728]

df = ewm_features(df, alphas, lags)

# Creating "Day of Year" lag features
df.sort_values(["day_of_year", "date"], inplace=True)

# Final sorting
df.sort_values(["date"], inplace=True)


# pd.cut
clus = df.groupby(["store"]).sales.mean().reset_index()
clus["store_cluster"] =  pd.cut(clus.sales, bins = 4, labels = range(1,5))
clus.drop("sales", axis = 1, inplace = True)
df = pd.merge(df, clus, how = "left")
clus = df.groupby(["item"]).sales.mean().reset_index()
clus["item_cluster"] =  pd.cut(clus.sales, bins = 5, labels = range(1,6))
clus.drop("sales", axis = 1, inplace = True)
df = pd.merge(df, clus, how = "left")
del clus

df.shape

df.head()

"""

# Train-Test Split"""

# Train-Test split

# Dataframe must be sorted by date because of Time Series Split
df = df.sort_values("date").reset_index(drop = True)

# Train Validation Split
# Validation set includes 3 months (Oct. Nov. Dec. 2017)

train = df.loc[(df["date"] < "2017-10-01"), :]
val = df.loc[(df["date"] >= "2017-10-01") & (df["date"] < "2018-01-01"), :]


cols = [col for col in train.columns if col not in ['date', 'id', "sales", "year"]]

Y_train = train['sales']
X_train = train[cols]

Y_test = val['sales']
X_test = val[cols]

Y_train.shape, X_train.shape, Y_test.shape, X_test.shape

"""# Metric Used SMAPE

Here is a small description:


The **symmetric mean absolute percentage error** (SMAPE or sMAPE) is an accuracy measure based on percentage (or relative) errors. It is usually defined as follows:

$$
\text{SMAPE} = \frac{100}{n} \sum_{t=1}^{n} \frac{|F_t - A_t|}{\left(\frac{|A_t| + |F_t|}{2}\right)}
$$

where:

- \( A_t \) is the actual value  
- \( F_t \) is the forecast value  

The absolute difference between \( A_t \) and \( F_t \) is divided by half the sum of the absolute values of the actual value \( A_t \) and the forecast value \( F_t \). The result is summed over all fitted points \( t \) and then divided by the total number of fitted points \( n \).

If you want to see the SMAPE formula, click [here](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error).
"""

# SMAPE: Symmetric mean absolute percentage error (adjusted MAPE)
def smape(preds, target):
    n = len(preds)
    masked_arr = ~((preds == 0) & (target == 0))
    preds, target = preds[masked_arr], target[masked_arr]
    num = np.abs(preds-target)
    denom = np.abs(preds)+np.abs(target)
    smape_val = (200*np.sum(num/denom))/n
    return smape_val

def lgbm_smape(y_true, y_pred):
    smape_val = smape(y_true, y_pred)
    return 'SMAPE', smape_val, False

"""#First Model

First Model With Default Parameters
"""

first_model = lgb.LGBMRegressor(random_state=384).fit(X_train, Y_train,
                                                      eval_metric= lambda y_true, y_pred: [lgbm_smape(y_true, y_pred)])

y_pred = first_model.predict(X_test)
print("TRAIN SMAPE:", smape(Y_train, first_model.predict(X_train)))
print("VALID SMAPE:", smape(Y_test, y_pred))

y_pred = first_model.predict(X_test)
print("TRAIN MAE:", mean_absolute_error(Y_train, first_model.predict(X_train)))
print("VALID MAE:", mean_absolute_error(Y_test, y_pred))

y_pred = first_model.predict(X_test)
print("TRAIN RMSE:", np.sqrt(mean_squared_error(Y_train, first_model.predict(X_train))))
print("VALID RMSE:", np.sqrt(mean_squared_error(Y_test, y_pred)))

print("MEAN:", np.mean(df['sales']))

"""# Feature Importance Analysis


The function plot_lgb_importances is designed to analyze and visualize feature importance for a trained LightGBM model. It helps in understanding which features contribute the most to the model's predictions.
"""

# feature importance

def plot_lgb_importances(model, plot=False, num=10):
    from matplotlib import pyplot as plt
    import seaborn as sns

    gain = model.booster_.feature_importance(importance_type='gain')
    feat_imp = pd.DataFrame({'feature': model.feature_name_,
                             'split': model.booster_.feature_importance(importance_type='split'),
                             'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)
    if plot:
        plt.figure(figsize=(10, 10))
        sns.set(font_scale=1)
        sns.barplot(x="gain", y="feature", data=feat_imp[0:25])
        plt.title('feature')
        plt.tight_layout()
        plt.show()
    else:
        print(feat_imp.head(num))
        return feat_imp

feature_imp_df = plot_lgb_importances(first_model, num=50)

plot_lgb_importances(first_model, plot=True, num=30)

"""# Plots actual vs predicted demand for each store and item in subplots.

  
"""

def show_plot(y_test, y_pred, store_item_df):

    df_plot = store_item_df.copy()
    df_plot["Actual"] = y_test
    df_plot["Predicted"] = y_pred

    stores = df_plot["store"].unique()

    store = stores[0]
    sub_df = df_plot[df_plot["store"] == store]
    items = sub_df["item"].unique()
    num_items = len(items)

    rows = (num_items // 5) + (num_items % 5 > 0)  # Auto-adjust row count
    fig, axes = plt.subplots(rows, 5, figsize=(20, 5 * rows))  # Grid layout
    axes = np.array(axes).flatten()  # Flatten for easy indexing

    for idx, item in enumerate(items):
        item_df = sub_df[sub_df["item"] == item].set_index("date")

        # Plot actual and predicted values
        item_df["Actual"].plot(ax=axes[idx], legend=True, label=f"Item {item} Sales", color="blue")
        item_df["Predicted"].plot(ax=axes[idx], legend=True, label=f"Item {item} Pred", linestyle="dashed", color="red")

        axes[idx].set_title(f"Store {store} - Item {item}")
        axes[idx].set_xlabel("Date")
        axes[idx].set_ylabel("Demand")
        axes[idx].grid(True)

    # Hide empty subplots
    for idx in range(num_items, len(axes)):
        fig.delaxes(axes[idx])

    plt.tight_layout(pad=4.5)
    plt.suptitle(f"Store {store} - Item Demand Distribution", fontsize=16)
    plt.show()

# Example usage
store_item_df = X_test[["store", "item"]].copy()
store_item_df["date"] = X_test.index  # Ensure date is an index column
show_plot(Y_test, y_pred, store_item_df)

# First model feature importance
print(len(feature_imp_df))
cols = feature_imp_df[feature_imp_df.gain > 0.05].feature.tolist()
print("Independent Variables:", len(cols))




second_model = lgb.LGBMRegressor(
    random_state=384,
    subsample=0.8,
    num_leaves=31,
    n_estimators=200,
    learning_rate=0.05,
    colsample_bytree=1.0
    ).fit(
    X_train[cols], Y_train,
    eval_metric= lambda y_true, y_pred: [lgbm_smape(y_true, y_pred)])

y_pred2 = second_model.predict(X_test[cols])
print("TRAIN SMAPE:", smape(Y_train, second_model.predict(X_train[cols])))
print("VALID SMAPE:", smape(Y_test, y_pred2))

y_pred2 = second_model.predict(X_test[cols])
print("TRAIN MAE:", mean_absolute_error(Y_train, second_model.predict(X_train[cols])))
print("VALID MAE:", mean_absolute_error(Y_test, y_pred2))

y_pred2 = second_model.predict(X_test[cols])
print("TRAIN RMSE:", np.sqrt(mean_squared_error(Y_train, second_model.predict(X_train[cols]))))
print("VALID RMSE:", np.sqrt(mean_squared_error(Y_test, y_pred2)))
print("MEAN:", np.mean(df['sales']))

# Example usage
store_item_df = X_test[["store", "item"]].copy()
store_item_df["date"] = X_test.index  # Ensure date is an index column
show_plot(Y_test, y_pred, store_item_df)

param_grid = {
    'num_leaves': [31, 50, 100],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'n_estimators': [100, 200, 500],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# Use TimeSeriesSplit to prevent data leakage
tscv = TimeSeriesSplit(n_splits=5)

random_search = RandomizedSearchCV(
    estimator=second_model,
    param_distributions=param_grid,
    n_iter=20,  # Adjust based on computational power
    cv=tscv,  # Use time-series-aware cross-validation
    verbose=1,
    n_jobs=-1,
    scoring='neg_mean_squared_error'
)

# Fit with correct early stopping parameters
random_search.fit(
    X_train[cols], Y_train,
    eval_set=[(X_test[cols], Y_test)],  # Validation set
    callbacks=[early_stopping(50)]
)
# Get the best parameters
print("Best Parameters:", random_search.best_params_)

best_model = random_search.best_estimator_
y_train_pred = best_model.predict(X_train[cols])
y_test_pred = best_model.predict(X_test[cols])

print("TRAIN SMAPE:", smape(Y_train, y_train_pred))

print("VALID SMAPE:", smape(Y_test, y_test_pred))

import pickle
import h5py
model_bytes = np.void(pickle.dumps(best_model))  # Convert to NumPy void type

# Save to HDF5
with h5py.File("lgbm_model.h5", "w") as f:
    f.create_dataset("model", data=model_bytes)

if hasattr(best_model, 'feature_name_'):
    print("Feature names:", best_model.feature_name_)
else:
    print("Feature names are not available.")

X_train[cols][-3:]

print("Model features:", best_model.booster_.feature_name())